"""
Asynchronous data acquisition module for the /deepanalyze command.
This module handles web research by searching DuckDuckGo and scraping relevant content.
"""

import asyncio
import aiohttp
from aiohttp import ClientSession, ClientError
from bs4 import BeautifulSoup
import time
import random
import logging
import os
from typing import List, Dict, Optional, Any
from urllib.parse import urlparse
from collections import defaultdict
import re

logger = logging.getLogger(__name__)

# Optional Flash reranker
try:
    import google.generativeai as genai
except Exception:
    genai = None
# Use the correct DDGS import
try:
    from ddgs import DDGS
except ImportError:
    try:
        from duckduckgo_search import DDGS
    except ImportError:
        # Fallback to the older duckduckgo_search library name
        from duckduckgo_search import ddg as DDGS
import re

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Constants for concurrency control and timeouts (configurable via environment variables)
MAX_CONCURRENT_REQUESTS = int(os.getenv('DATA_ACQUISITION_MAX_CONCURRENT_REQUESTS', '5'))
REQUEST_DELAY_MIN = float(os.getenv('DATA_ACQUISITION_REQUEST_DELAY_MIN', 0.5))
REQUEST_DELAY_MAX = float(os.getenv('DATA_ACQUISITION_REQUEST_DELAY_MAX', 2.0))
MAX_RETRIES = int(os.getenv('DATA_ACQUISITION_MAX_RETRIES', 3))
INITIAL_RETRY_DELAY = float(os.getenv('DATA_ACQUISITION_INITIAL_RETRY_DELAY', 1.0))
REQUEST_TIMEOUT = float(os.getenv('DATA_ACQUISITION_REQUEST_TIMEOUT', '30.0'))  # More flexible default timeout
ADAPTIVE_TIMEOUT_MULTIPLIER = float(os.getenv('DATA_ACQUISITION_ADAPTIVE_TIMEOUT_MULTIPLIER', '1.5'))  # For adaptive timeouts

async def acquire_data(queries: List[str]) -> List[Dict[str, str]]:
    """
    Acquire data from web sources based on search queries.
    
    Args:
        queries: List of search queries generated by the query generator
        
    Returns:
        List of dictionaries with structure: {'url': url, 'text': scraped_text}
    """
    # Create a semaphore to limit concurrent requests
    semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)
    
    # Build a deduplicated list of search query strings (base + reputable site-specific), with global caps
    reputable = os.getenv('REPUTABLE_DOMAINS', 'bbc.com,espn.com,skysports.com,theguardian.com,cbssports.com,foxsports.com,nbcsports.com,si.com,reuters.com,apnews.com,whoscored.com,transfermarkt.com,uefa.com,fifa.com,premierleague.com,bundesliga.com,laliga.com,seriea.com,nba.com,nfl.com,mlb.com,nhl.com').split(',')
    reputable = [d.strip() for d in reputable if d.strip()]
    per_domain_queries = int(os.getenv('PER_DOMAIN_QUERIES', '1'))
    top_domains_per_query = int(os.getenv('TOP_DOMAINS_PER_QUERY', '6'))
    max_search_queries_total = int(os.getenv('MAX_SEARCH_QUERIES_TOTAL', '40'))
    search_max_concurrent = int(os.getenv('SEARCH_MAX_CONCURRENT', '6'))

    query_strings: List[str] = []
    # Start with base queries
    for q in queries:
        if q not in query_strings:
            query_strings.append(q)
        # Add site-specific queries for top reputable domains
        for d in reputable[:top_domains_per_query]:
            site_q = f"{q} site:{d}"
            if site_q not in query_strings:
                query_strings.append(site_q)

    # Cap total queries
    if len(query_strings) > max_search_queries_total:
        query_strings = query_strings[:max_search_queries_total]

    logger.info(f"Search planning | base={len(queries)} site_augmented={len(query_strings)} max_total={max_search_queries_total} top_domains_per_query={top_domains_per_query}")

    # Execute searches with limited concurrency
    serp_items: List[Dict[str, Any]] = []
    _search_sem = asyncio.Semaphore(search_max_concurrent)

    async def _search_one(qstr: str) -> List[Dict[str, Any]]:
        async with _search_sem:
            try:
                # Disable retries for individual search calls; weâ€™ll decide globally later
                return await _search_duckduckgo(qstr, max_results=per_domain_queries if ' site:' in qstr else 5, do_retries=False)
            except Exception as e:
                logger.warning(f"Search error for '{qstr}': {e}")
                return []

    search_tasks = [asyncio.create_task(_search_one(qs)) for qs in query_strings]
    search_results = await asyncio.gather(*search_tasks, return_exceptions=False)

    total_calls = len(search_results)
    succeeded = sum(1 for lst in search_results if isinstance(lst, list) and len(lst) > 0)
    failed = total_calls - succeeded

    # If more than half failed, perform a single global retry for failed queries only (with retries enabled)
    if failed > total_calls / 2 and total_calls > 0:
        logger.warning(f"High failure rate in searches (failed={failed}/{total_calls}), retrying failed queries once...")
        retry_tasks = []
        for idx, lst in enumerate(search_results):
            if not isinstance(lst, list) or len(lst) == 0:
                retry_tasks.append(asyncio.create_task(_search_duckduckgo(query_strings[idx], max_results=per_domain_queries if ' site:' in query_strings[idx] else 5, do_retries=True)))
            else:
                retry_tasks.append(asyncio.create_task(asyncio.sleep(0, result=lst)))  # keep success as-is
        retry_results = await asyncio.gather(*retry_tasks, return_exceptions=False)
        search_results = retry_results

    for lst in search_results:
        if isinstance(lst, list):
            serp_items.extend(lst)

    logger.info(f"Search completed | queries_executed={len(query_strings)} succ={succeeded} fail={failed} serp_items={len(serp_items)}")

    # Phase 2 default: Flash reranking is required; no fallback.
    if genai is None or not os.getenv("GEMINI_API_KEY"):
        raise RuntimeError("SERP reranker required but GEMINI_API_KEY or client is unavailable")
    reranked = _rerank_serp_with_flash(serp_items, queries)
    logger.info(f"SERP reranker USED | items_in={len(serp_items)} items_out={len(reranked)}")

    # Pre-fetch filtering: dedupe by URL, enforce whitelist minimum, prefer higher rerank_score, keep diversity
    whitelist = [
        'bbc.co.uk', 'bbc.com', 'espn.com', 'skysports.com', 'theguardian.com',
        'whoscored.com', 'transfermarkt.com', 'uefa.com', 'premierleague.com',
        'bundesliga.com', 'laliga.com', 'seriea.com', 'as.com', 'marca.com', 'goal.com',
        'liverpoolfc.com', 'arsenal.com', 'uefa.com', 'fifa.com'
    ]

    def is_whitelisted(url: str) -> bool:
        host = urlparse(url).hostname or ''
        return any(host.endswith(w) for w in whitelist)

    max_per_domain = int(os.getenv('MAX_PER_DOMAIN', '2'))
    max_fetches = int(os.getenv('MAX_FETCHES', '12'))
    min_whitelist = int(os.getenv('MIN_WHITELIST_COUNT', '3'))

    seen: set = set()
    per_host_count: Dict[str, int] = {}
    whitelisted_first: List[str] = []
    others: List[str] = []

    # Sort primarily by rerank_score descending, then stable
    reranked_sorted = sorted(reranked, key=lambda x: x.get('rerank_score', 0), reverse=True)

    for item in reranked_sorted:
        url = item.get('url', '')
        if not url or url in seen:
            continue
        host = urlparse(url).hostname or ''
        if per_host_count.get(host, 0) >= max_per_domain:
            continue
        if is_whitelisted(url):
            whitelisted_first.append(url)
        else:
            others.append(url)
        per_host_count[host] = per_host_count.get(host, 0) + 1
        seen.add(url)

    # Enforce at least min_whitelist, then fill with others, capped by max_fetches
    filtered_urls: List[str] = []
    filtered_urls.extend(whitelisted_first[:min_whitelist])
    if len(filtered_urls) < max_fetches:
        filtered_urls.extend(others[: max_fetches - len(filtered_urls)])

    # Cap (again for safety)
    filtered_urls = filtered_urls[:max_fetches]

    logger.info(f"URL selection | whitelisted={len(whitelisted_first)} others={len(others)} selected={len(filtered_urls)} min_whitelist={min_whitelist} max_fetches={max_fetches}")

    # Fetch content from URLs concurrently
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_and_parse_url(session, url, semaphore) for url in filtered_urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)

    # Filter out exceptions and None results
    valid_results = []
    for result in results:
        if isinstance(result, Exception):
            logger.warning(f"Error fetching URL: {result}")
        elif result is not None:
            valid_results.append(result)

    # Filter out irrelevant content
    filtered_results = filter_content(valid_results)

    # Ensure we return up to the target number of articles, padding with remaining valid ones if needed
    target_count = int(os.getenv('TARGET_ARTICLES', '10'))
    if target_count <= 0:
        target_count = 10

    # Start from filtered (higher quality)
    selected = list(filtered_results[:target_count])

    if len(selected) < target_count:
        # Pad with remaining valid (unfiltered) results not already selected
        selected_urls = {item['url'] for item in selected}
        for item in valid_results:
            if item and item.get('url') and item['url'] not in selected_urls:
                selected.append(item)
                selected_urls.add(item['url'])
                if len(selected) >= target_count:
                    break

    # Final cap to target_count to avoid oversized payloads downstream
    selected = selected[:target_count]

    logger.info(f"Articles selected | valid={len(valid_results)} filtered={len(filtered_results)} selected={len(selected)} target={target_count}")

    return selected


def _recency_hint_score(text: str) -> int:
    if not text:
        return 0
    txt = text.lower()
    score = 0
    # Simple recent indicators
    for kw in ['today', 'this week', 'hour ago', 'hours ago', 'yesterday']:
        if kw in txt:
            score += 10
    # Year or month hints
    year_match = re.search(r'\b20(2[4-9]|3[0-5])\b', txt)
    if year_match:
        score += 5
    for mon in ['jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec']:
        if mon in txt:
            score += 3
    return score

def _rerank_serp_with_flash(serp_items: List[Dict[str, Any]], queries: List[str]) -> List[Dict[str, Any]]:
    """Rerank SERP items using Gemini Flash 2.5 and attach a rerank_score per item.
    Higher rerank_score means higher priority.
    """
    try:
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key or genai is None or not serp_items:
            return serp_items
        genai.configure(api_key=api_key)
        model_id = os.getenv("GEMINI_FLASH_MODEL_ID") or "gemini-2.5-flash"
        model = genai.GenerativeModel(model_id)
        # Prepare a compact prompt to minimize tokens
        lines = []
        for it in serp_items[:30]:  # limit prompt size
            lines.append(f"URL: {it.get('url','')}\nTITLE: {it.get('title','')}\nSNIPPET: {it.get('snippet','')}")
        prompt = (
            "Rank these results for a sports match research task. Prefer reputable domains (BBC, ESPN, Sky, Guardian, WhoScored, Transfermarkt, official league/club), recency hints in snippet, and clear relevance to the event queries.\n"
            "Return top items (up to 20) as lines 'URL'. Do not add explanations.\n\n" +
            "\n\n".join(lines) +
            "\n\nEvent Queries:\n" + "\n".join(queries[:5])
        )
        resp = model.generate_content(prompt)
        text = getattr(resp, "text", "") or ""
        ranked_urls = [ln.strip() for ln in text.splitlines() if ln.strip().startswith('http')]
        by_url = {it.get('url',''): dict(it) for it in serp_items}
        # Assign scores: highest rank gets highest score
        score_start = 100
        scores = {}
        for idx, u in enumerate(ranked_urls):
            scores[u] = score_start - idx
        # Build ranked list with scores and add domain/recency boosts
        DOMAIN_WEIGHT = float(os.getenv('DOMAIN_WEIGHT', '15'))
        RECENCY_WEIGHT = float(os.getenv('RECENCY_WEIGHT', '10'))
        whitelist = os.getenv('REPUTABLE_DOMAINS', 'bbc.com,espn.com,skysports.com,theguardian.com,whoscored.com,transfermarkt.com,uefa.com,premierleague.com,bundesliga.com,laliga.com,seriea.com').split(',')
        whitelist = [w.strip() for w in whitelist if w.strip()]

        ranked: List[Dict[str, Any]] = []
        seen = set()
        for u in ranked_urls:
            if u in by_url:
                item = by_url[u]
                base = scores.get(u, 0)
                host = urlparse(u).hostname or ''
                is_white = any(host.endswith(w) for w in whitelist)
                domain_boost = DOMAIN_WEIGHT if is_white else 0
                recency_boost = RECENCY_WEIGHT if (_recency_hint_score(item.get('title','')) + _recency_hint_score(item.get('snippet','')))>0 else 0
                item['rerank_score'] = base + domain_boost + recency_boost
                ranked.append(item)
                seen.add(u)
        # Append remaining with low scores, preserving original order and boost if applicable
        for it in serp_items:
            u = it.get('url','')
            if u not in seen:
                item = dict(it)
                base = scores.get(u, 0)
                host = urlparse(u).hostname or ''
                is_white = any(host.endswith(w) for w in whitelist)
                domain_boost = DOMAIN_WEIGHT if is_white else 0
                recency_boost = RECENCY_WEIGHT if (_recency_hint_score(item.get('title','')) + _recency_hint_score(item.get('snippet','')))>0 else 0
                item['rerank_score'] = base + domain_boost + recency_boost
                ranked.append(item)
                seen.add(u)
        return ranked
    except Exception as e:
        logger.warning(f"Flash reranking failed: {e}")
        # If reranking fails, at least attach neutral scores to preserve pipeline expectations
        ranked = []
        for it in serp_items:
            item = dict(it)
            item['rerank_score'] = 0
            ranked.append(item)
        return ranked

async def _search_duckduckgo(query: str, max_results: int = 5, do_retries: bool = True) -> List[Dict[str, Any]]:
    """
    Search DuckDuckGo for a query and return top results.
    
    Args:
        query: Search query string
        max_results: Maximum number of results to return
        
    Returns:
        List of URLs
    """
    items: List[Dict[str, Any]] = []
    retries = 0
    
    while True:
        try:
            ddgs = DDGS()
            results = ddgs.text(query, max_results=max_results)
            for r in results:
                href = r.get('href') if isinstance(r, dict) else r
                if not href:
                    continue
                items.append({
                    'url': href,
                    'title': r.get('title', '') if isinstance(r, dict) else '',
                    'snippet': r.get('body', '') if isinstance(r, dict) else ''
                })
            break
        except Exception as e:
            if not do_retries:
                logger.warning(f"DuckDuckGo search failed (no retry): {e}")
                break
            retries += 1
            if retries > MAX_RETRIES:
                logger.error(f"Failed to search DuckDuckGo after {MAX_RETRIES} retries: {e}")
                break
            delay = INITIAL_RETRY_DELAY * (2 ** (retries - 1))
            logger.warning(f"DuckDuckGo search failed, retrying in {delay} seconds: {e}")
            await asyncio.sleep(delay)
    
    return items

async def fetch_and_parse_url(session: ClientSession, url: str, semaphore: asyncio.Semaphore) -> Optional[Dict[str, str]]:
    """Fetch and parse content with per-domain politeness, adaptive timeouts, and robust error handling."""
    async with semaphore:
        # Politeness: small jitter before requests
        await asyncio.sleep(random.uniform(REQUEST_DELAY_MIN, REQUEST_DELAY_MAX))

        timeout = REQUEST_TIMEOUT
        retries = 0
        consecutive_429 = 0

        while retries <= MAX_RETRIES:
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                }
                adaptive_timeout = timeout * (ADAPTIVE_TIMEOUT_MULTIPLIER ** retries)
                aiohttp_timeout = aiohttp.ClientTimeout(total=adaptive_timeout)

                async with session.get(url, headers=headers, timeout=aiohttp_timeout) as response:
                    if response.status == 200:
                        content = await response.text(errors='ignore')
                        text = _extract_text_from_html(content)
                        if text and len(text) >= 100:
                            return {'url': url, 'text': text}
                        logger.warning(f"Insufficient or no text extracted from {url}")
                        return None
                    if response.status == 429:  # Rate limited
                        consecutive_429 += 1
                        retries += 1
                        if retries > MAX_RETRIES:
                            logger.error(f"Rate limited for {url} after {MAX_RETRIES} retries")
                            return None
                        backoff = INITIAL_RETRY_DELAY * (2 ** (retries - 1)) * (1 + consecutive_429 * 0.25)
                        logger.warning(f"429 from {url}, backoff {backoff:.1f}s")
                        await asyncio.sleep(backoff)
                        continue
                    if response.status in (403, 404):
                        logger.warning(f"HTTP {response.status} for {url}")
                        return None
                    logger.warning(f"HTTP {response.status} for {url}")
                    return None

            except asyncio.CancelledError:
                # Treat cancellation like a timeout; do not bubble up
                retries += 1
                if retries > MAX_RETRIES:
                    logger.error(f"Cancelled while fetching {url} after retries")
                    return None
                await asyncio.sleep(INITIAL_RETRY_DELAY * (2 ** (retries - 1)))
                continue
            except asyncio.TimeoutError:
                retries += 1
                if retries > MAX_RETRIES:
                    logger.error(f"Timeout fetching {url} after {MAX_RETRIES} retries (timeout={adaptive_timeout:.1f}s)")
                    return None
                backoff = INITIAL_RETRY_DELAY * (2 ** (retries - 1))
                logger.warning(f"Timeout fetching {url} with timeout {adaptive_timeout:.1f}s, retrying in {backoff:.1f}s")
                await asyncio.sleep(backoff)
                continue
            except ClientError as e:
                logger.warning(f"Network error fetching {url}: {e}")
                return None
            except Exception as e:
                logger.error(f"Unexpected error fetching {url}: {e}")
                return None

    return None

def _extract_text_from_html(html: str) -> str:
    """
    Extract clean text content from HTML.
    
    Args:
        html: HTML content
        
    Returns:
        Clean text content
    """
    try:
        soup = BeautifulSoup(html, 'html.parser')
        
        # Remove script and style elements
        for script in soup(["script", "style"]):
            script.decompose()
        
        # Get text and clean it up
        text = soup.get_text()
        
        # Break into lines and remove leading/trailing space
        lines = (line.strip() for line in text.splitlines())
        
        # Break multi-headlines into a line each
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        
        # Drop blank lines
        text = ' '.join(chunk for chunk in chunks if chunk)
        
        return text
    except Exception as e:
        logger.error(f"Error parsing HTML: {e}")
        return ""

def filter_content(results: List[Dict[str, str]]) -> List[Dict[str, str]]:
    """
    Filter out irrelevant or low-quality content.
    
    Args:
        results: List of dictionaries with 'url' and 'text' keys
        
    Returns:
        Filtered list of results
    """
    filtered = []
    
    for result in results:
        url = result.get('url', '')
        text = result.get('text', '')
        
        # Skip if no text
        if not text:
            continue
            
        # Skip if too little content (less than 100 characters)
        if len(text) < 100:
            continue
            
        # Skip if clearly not about sports
        if _is_irrelevant_content(text):
            continue
            
        # Skip if clearly not about the teams/players we're interested in
        # (This would ideally be more sophisticated and based on the original query)
        
        filtered.append(result)
    
    return filtered

def _is_irrelevant_content(text: str) -> bool:
    """
    Check if content is clearly irrelevant to sports.
    
    Args:
        text: Text content to check
        
    Returns:
        True if content is irrelevant, False otherwise
    """
    # Convert to lowercase for easier matching
    text_lower = text.lower()
    
    # Keywords that suggest irrelevant content
    irrelevant_keywords = [
        'advertisement', 'advertisements', 'click here', 'buy now', 'shop now',
        'privacy policy', 'terms of service', 'cookie policy', 'subscribe',
        'sign up', 'register', 'login', 'account', 'password',
        '404 error', 'page not found', 'file not found'
    ]
    
    # Check for irrelevant keywords
    for keyword in irrelevant_keywords:
        if keyword in text_lower:
            return True
    
    # Check if it's mostly non-alphabetic (likely not real content)
    if len(text) > 0:
        alpha_chars = sum(1 for c in text if c.isalpha())
        if alpha_chars / len(text) < 0.5:
            return True
    
    return False

# Example usage
if __name__ == "__main__":
    async def main():
        # Example queries that might be generated by the query generator
        queries = [
            "Manchester United vs Liverpool team news",
            "Manchester United vs Liverpool prediction",
            "Manchester United recent form and results",
            "Liverpool player injuries and suspensions"
        ]
        
        results = await acquire_data(queries)
        print(f"Acquired {len(results)} results:")
        for i, result in enumerate(results, 1):
            print(f"{i}. {result['url'][:80]}...")
            # Show first 200 characters of text, handling encoding issues
            try:
                text_preview = result['text'][:200].encode('ascii', 'ignore').decode('ascii')
                print(f"   Text preview: {text_preview}...")
            except Exception as e:
                print(f"   Error displaying text preview: {e}")
            print()
    
    # Run the example
    asyncio.run(main())